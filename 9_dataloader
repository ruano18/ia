import torch
import torchvision
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math

#---------------------------------------------------
# cálculo de gradiente, etc. no es eficiente para todo el conjunto de datos
# -> dividir el conjunto de datos en lotes pequeños
#---------------------------------------------------
#---------------------------------------------------
# epoch = un pase hacia adelante y hacia atrás de TODAS las muestras de entrenamiento
# lote_tamaño = número de muestras de entrenamiento utilizadas en un pase de avance/retroceso
# número de iteraciones = número de pases, cada pase (adelante+atrás) usando [batch_size] número de muestras
# por ejemplo: 100 muestras, batch_size=20 -> 100/20=5 iteraciones para 1 época

# --> DataLoader puede hacer el cálculo por lotes por nosotros

# Implementar un conjunto de datos personalizado:
# heredar conjunto de datos
# implementar __init__, __getitem__ y __len__
#---------------------------------------------------

class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

#---------------------------------------------------
# crear base de datos
#---------------------------------------------------
dataset = WineDataset()

#---------------------------------------------------
# primera muestra
#---------------------------------------------------
first_data = dataset[0]
features, labels = first_data
print(features, labels)

#---------------------------------------------------
# Cargar toda la base de datos con DataLoader
# shuffle: datos aleatorios, bueno para entrenamiento
# num_workers: carga más rápida con múltiples subprocesos
# !!! SI RECIBE UN ERROR DURANTE LA CARGA, CONFIGURE num_workers EN 0 !!!
#---------------------------------------------------
train_loader = DataLoader(dataset=dataset,
                          batch_size=4,
                          shuffle=True,
                          num_workers=2)

#---------------------------------------------------
# convertir a iterador y ver la muestra random
#---------------------------------------------------
dataiter = iter(train_loader)
data = next(dataiter)
features, labels = data
print(features, labels)

#---------------------------------------------------
# Dummy ciclo de aprendizaje
#---------------------------------------------------
num_epochs = 2
total_samples = len(dataset)
n_iterations = math.ceil(total_samples/4)
print(total_samples, n_iterations)
for epoch in range(num_epochs):
    for i, (inputs, labels) in enumerate(train_loader):
        
        #ejemplo: 178 muestras, batch_size = 4, n_iters=178/4=44.5 -> 45 iteraciones
  #---------------------------------------------------
  # correr el proceso de aprendizaje
  #---------------------------------------------------
        if (i+1) % 5 == 0:
            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')
            

train_dataset = torchvision.datasets.MNIST(root='./data', 
                                           train=True, 
                                           transform=torchvision.transforms.ToTensor(),  
                                           download=True)

train_loader = DataLoader(dataset=train_dataset, 
                                           batch_size=3, 
                                           shuffle=True)

# look at one random sample
dataiter = iter(train_loader)
data = next(dataiter)
inputs, targets = data
print(inputs.shape, targets.shape)
