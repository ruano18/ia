#-----------------------------------------------------
# diseñar el modelo (entrada, salida, NN con muchas capas)
# definir error y optimizador
# ciclos de aprendizaje
#    -forward = evaluar, predecir y calcular error
#    -backward = calcular gradiente
#    -mejorar coeficientes
#-----------------------------------------------------

import torch
import torch.nn as nn

#-----------------------------------------------------
#regresion lineal
# f = w * x 
#-----------------------------------------------------

# elemplo : f = 2 * x
#-----------------------------------------------------

#-----------------------------------------------------
# 0) datos de entrenamiento
#-----------------------------------------------------
X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)

#-----------------------------------------------------
# 1) diseño de modelo: coeficientes y NN
#-----------------------------------------------------
w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

def forward(x):
    return w * x

print(f'Prediction before training: f(5) = {forward(5).item():.3f}')

#-----------------------------------------------------
# 2) definir error y optimizador
#-----------------------------------------------------
learning_rate = 0.01
n_iters = 100
# error (loss) definido en pytorch
loss = nn.MSELoss()
#optimizador (SGD stochastic gradient descent)
optimizer = torch.optim.SGD([w], lr=learning_rate)

#-----------------------------------------------------
# 3) ciclo de aprendizaje
#-----------------------------------------------------
for epoch in range(n_iters):
    # predict = evaluar funcion
    y_predicted = forward(X)

    # error
    l = loss(Y, y_predicted)

    # calcular gradiente = retropropagacion
    l.backward()

    # mejorar coeficientes
    optimizer.step()

    # resetear gradiente
    optimizer.zero_grad()
    # diagnostico
    if epoch % 10 == 0:
        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)

print(f'prediccion despues del gradiente: f(5) = {forward(5).item():.3f}')
